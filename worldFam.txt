from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Create Spark session
spark = SparkSession.builder.appName("table_merge_logic").getOrCreate()

# Assume sampleaData_df and span_df are your DataFrames with the respective columns

# Step 1: Append span_df values to sampleaData_df as new columns and create table3
table3 = sampleaData_df.join(span_df, sampleaData_df.MBINumber == span_df.MBINumber, "left_outer") \
    .select(sampleaData_df["*"], span_df["Stype"], span_df["S_EffDate"], span_df["S_TermDate"])

# Step 2: Add BLEI_ConcatResult column based on the described logic
table3 = table3.withColumn("BLEI_ConcatResult",
    F.when((F.col("Stype") != "MCAID") & ((F.col("Prem_Withhold_opt").isNotNull()) | (F.col("Prem_Withhold_opt") != "N")) &
           ((F.col("PartCPremium") + F.col("PartDPremium")) > 0),
           F.expr("""concat('@pBLPF_ID = "S4", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', termdate, '"')""")
          )
          .when((F.col("Stype") == "MCAID") & (F.col("S_EffDate") <= F.col("effectivedate")) & (F.col("S_TermDate") >= F.col("termdate")),
           F.expr("""concat('@pBLPF_ID = "M2", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', termdate, '"')""")
          )
          .when((F.col("Stype") == "MCAID") & (F.col("S_EffDate") <= F.col("effectivedate")) & (F.col("S_TermDate") < F.col("termdate")),
           F.expr("""concat('@pBLPF_ID = "M2", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', S_TermDate, '"')""")
          )
          .when((F.col("Stype") == "MCAID") & (F.col("S_EffDate") > F.col("effectivedate")) & (F.col("S_TermDate") >= F.col("termdate")),
           F.expr("""concat('@pBLPF_ID = "S1", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', date_sub(S_EffDate, 1), '"')""")
          )
          .when((F.col("Stype") == "MCAID") & (F.col("S_EffDate") > F.col("effectivedate")) & (F.col("S_TermDate") < F.col("termdate")),
           F.expr("""concat('@pBLPF_ID = "S1", @pBLBP_EFF_DT = "', date_add(S_TermDate, 1), '", @pBLBP_TERM_DT = "', termdate, '"')""")
          )
          .otherwise(F.expr("""concat('@pBLPF_ID = "S1", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', termdate, '"')""")
)

# Show the final DataFrame
table3.show(truncate=False)

# Stop the Spark session
spark.stop()
--===================
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat_ws, col, lit, when

# Create SparkSession
spark = SparkSession.builder \
    .appName("Conditional Key-Value Pairs") \
    .getOrCreate()

# Assuming you have already loaded your input data into a DataFrame named 'nmdm_dftable'

# Define your key-value pairs with conditions
concat_expr = concat_ws(",", 
                        lit("@ppbpId="), col("pdbId"), 
                        lit(",@pSegmentId="), col("segmentId"), 
                        lit(",@pA_EffDate="), 
                        when(col("AeffDate").isNotNull(), col("AeffDate")).otherwise(lit("")), 
                        lit(",@pB_EffDate="), 
                        when(col("BeffDate").isNotNull(), col("BeffDate")).otherwise(lit("")), 
                        lit(",@pC_EffDate="), 
                        when(col("CeffDate").isNotNull(), col("CeffDate")).otherwise(lit(""))
                        # Add similar conditions for other optional columns
                        )

# Select subID as SID and the concatenated result with conditional key-value pairs
result_df = nmdm_dftable.select(col("subID").alias("SID"), concat_expr.alias("ConcatResult"))

# Now you can proceed to create a table from 'result_df' or perform further operations as needed


=================
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("SplitName") \
    .getOrCreate()

# Sample DataFrame
data = [("John A. Doe", 1),
        ("Jane, A. Smith", 2),
        ("Alice B. Bob", 3)]

columns = ["res_party_name", "Id"]
df = spark.createDataFrame(data, columns)

# Create a temporary view to execute SQL queries
df.createOrReplaceTempView("table1")

# Splitting res_party_name into firstname, middlename, and lastname based on space or comma using Spark SQL
result_df = spark.sql("""
    SELECT 
        CONCAT('@pID="', Id, '",@pFirstname="', 
               CASE 
                   WHEN INSTR(res_party_name, ',') > 0 THEN SUBSTRING_INDEX(res_party_name, ',', 1)
                   ELSE SPLIT(res_party_name, ' ')[0]
               END,
               '",@pMiddlename="', 
               CASE 
                   WHEN INSTR(res_party_name, ',') > 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(res_party_name, ',', -1), ' ', -1)
                   ELSE CASE WHEN SIZE(SPLIT(res_party_name, ' ')) > 2 THEN SPLIT(res_party_name, ' ')[1] ELSE '' END
               END,
               '",@pLastname="', 
               CASE 
                   WHEN INSTR(res_party_name, ',') > 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(res_party_name, ',', -1), ' ', 1)
                   ELSE CASE WHEN SIZE(SPLIT(res_party_name, ' ')) > 2 THEN SPLIT(res_party_name, ' ')[2] ELSE SPLIT(res_party_name, ' ')[1] END
               END,
               '"') AS ConcatResult
    FROM 
        table1
""")

# Show the result
result_df.show(truncate=False)

# Stop SparkSession
spark.stop()
-----============

from pyspark.sql.functions import *

# Pre-populate CSPI_ID data
sd_df_with_CSPI_ID = spark.sql("""
    SELECT 
        sb.*,
        pc.CSPI_ID
    FROM 
        sd_dfTable sb
    INNER JOIN 
        pc_dftable pc 
    ON 
        sb.planid = pc.contractid AND sb.pbp = pc.pbp
""")

# Now, you can use sd_df_with_CSPI_ID to build SBEL_df
SBEL_df = spark.sql("""
    SELECT 
        sd.sbsb_id AS SID,
        CASE
            WHEN m.sbsb_id IS NULL THEN CONCAT('@pUpdate_CD="AP’,"@pSBEL_EffDate="', sd.effectivedate, '",@pSBEL_ELIG_TYPE="SL"')
            WHEN m.sbsb_id IS NOT NULL AND sd.effectivedate BETWEEN m.mepe_effectivedate AND m.mepe_termdate AND sd.CSPI_ID = m.CSPI_ID AND sd.CSPD_CAT = m.CSPD_CAT THEN NULL
            WHEN m.sbsb_id IS NOT NULL AND sd.effectivedate BETWEEN m.mepe_effectivedate AND m.mepe_termdate AND (sd.CSPI_ID != m.CSPI_ID OR sd.CSPD_CAT != m.CSPD_CAT) THEN CONCAT('@pUpdate_CD="AP’,"@pSBEL_EffDate="', sd.effectivedate, '",@pSBEL_ELIG_TYPE="CH"')
            WHEN NOT EXISTS (SELECT * FROM mepe_df WHERE sd.sbsb_id = m.sbsb_id AND sd.effectivedate BETWEEN m.mepe_effectivedate AND m.mepe_termdate) THEN CONCAT('@pUpdate_CD="AP’,"@pSBEL_EffDate="', sd.effectivedate, '",@pSBEL_ELIG_TYPE="RI"')
            ELSE NULL
        END AS SBEL_ConcatResult
    FROM 
        sd_df_with_CSPI_ID sd
    LEFT JOIN 
        mepe_df m ON sd.sbsb_id = m.sbsb_id
""")

# Handling term date condition
SBEL_df = SBEL_df.union(spark.sql("""
    SELECT 
        sd.sbsb_id AS SID,
        CONCAT('@pUpdate_CD="AP’,"@pSBEL_EffDate="', sd.termdate, '",@pSBEL_ELIG_TYPE="TM"') AS SBEL_ConcatResult
    FROM 
        sd_df_with_CSPI_ID sd
    JOIN 
        mepe_df m ON sd.sbsb_id = m.sbsb_id
    WHERE 
        (sd.termdate BETWEEN m.mepe_effectivedate AND m.mepe_termdate) OR (sd.termdate > m.mepe_termdate)
"""))
----===============================================

from pyspark.sql import SparkSession

# Assuming the Spark session is already created and named as 'spark'
# and DataFrames named sd_dfTable, mepe_df, sbel_df are defined

# Register the DataFrames as SQL temporary views
sd_dfTable.createOrReplaceTempView("sd_df")
mepe_df.createOrReplaceTempView("mepe_df")
sbel_df.createOrReplaceTempView("sbel_df")

# SQL query to generate keywords including the "else" condition
sql_query = """
SELECT 
  sd.sbsb_id AS SID,
  CONCAT(
    CASE
      WHEN sd.effectivedate = sd.termdate AND EXISTS (
        SELECT 1
        FROM mepe_df mepe
        WHERE mepe.sbsb_id = sd.sbsb_id 
          AND mepe.mepe_effectivedate <= sd.effectivedate 
          AND sd.effectivedate <= mepe.mepe_termdate
      ) THEN CONCAT(
        '@pSBEL_EffDate=', CAST(DATE_SUB(sd.effectivedate, 1) AS STRING), ',@pSBEL_ELIG_TYPE="TM"',
        '\r\n',
        (
          SELECT CONCAT_WS('\r\n', COLLECT_LIST(
            CONCAT('@pSBEL_Update_CD="VC",@pSBEL_EffDate=', CAST(sbel.SBEL_EFF_DT AS STRING), ',@pSBE_MCTR_VRSN_NVL="CE"')
          ))
          FROM sbel_df sbel
          WHERE sbel.sbsb_id = sd.sbsb_id 
            AND sbel.SBEL_EFF_DT >= sd.effectivedate
            AND sbel.SBEL_VOID_IND = 'N'
        )
      )
      
      WHEN sd.termdate > (
        SELECT MAX(mepe.mepe_termdate)
        FROM mepe_df mepe
        WHERE mepe.sbsb_id = sd.sbsb_id
      ) THEN CONCAT(
        '@pSBEL_EffDate=', CAST(sd.termdate AS STRING), ',@pSBEL_ELIG_TYPE="TM"',
        '\r\n',
        (
          SELECT CONCAT_WS('\r\n', COLLECT_LIST(
            CONCAT('@pSBEL_Update_CD="VC",@pSBEL_EffDate=', CAST(sbel.SBEL_EFF_DT AS STRING), ',@pSBE_MCTR_VRSN_NVL="CE"')
          ))
          FROM sbel_df sbel
          WHERE sbel.sbsb_id = sd.sbsb_id 
            AND sbel.SBEL_EFF_DT >= sd.termdate
            AND sbel.SBEL_VOID_IND = 'N'
        )
      )
      
      WHEN EXISTS (
        SELECT 1
        FROM mepe_df mepe
        WHERE mepe.sbsb_id = sd.sbsb_id 
          AND mepe.mepe_effectivedate <= sd.termdate 
          AND sd.termdate <= mepe.mepe_termdate
      ) THEN CONCAT('@pSBEL_EffDate=', CAST(sd.termdate AS STRING), ',@pSBEL_ELIG_TYPE="TM"')
      
      WHEN EXISTS (
        SELECT 1
        FROM mepe_df mepe
        WHERE mepe.sbsb_id = sd.sbsb_id 
          AND mepe.mepe_effectivedate <= sd.effectivedate 
          AND sd.effectivedate <= mepe.mepe_termdate
          AND (mepe.CSPI_ID != sd.CSPI_ID OR mepe.CSCS_ID != sd.CSCS_ID)
      ) THEN CONCAT('@pSBEL_EffDate=', CAST(sd.effectivedate AS STRING), ',@pSBEL_ELIG_TYPE="CH"')
      
      WHEN NOT EXISTS (
        SELECT 1
        FROM mepe_df mepe
        WHERE mepe.sbsb_id = sd.sbsb_id 
          AND (sd.effectivedate < mepe.mepe_effectivedate OR sd.effectivedate > mepe.mepe_termdate)
      ) THEN CONCAT('@pSBEL_EffDate=', CAST(sd.effectivedate AS STRING), ',@pSBEL_ELIG_TYPE="RI"')
      
    ELSE CONCAT('@pRECTYPE="SBEL",@pSBEL_Update_CD="IN",', '@pSBEL_EffDate="', CAST(sd.effectivedate AS STRING), '",@pSBEL_ELIG_TYPE="SL",', '@pCSPD_CAT="M",', '@pCSPI_ID="', sd.CSPI_ID, '",@pSBEL_FI ="C"
---=========================================

from pyspark.sql import SparkSession

# Assuming the Spark session is already created and named as 'spark'
# and DataFrames named sd_dfTable, mepe_df, sbel_df are defined

# Register the DataFrames as SQL temporary views
sd_dfTable.createOrReplaceTempView("sd_df")
mepe_df.createOrReplaceTempView("mepe_df")
sbel_df.createOrReplaceTempView("sbel_df")

# SQL query to generate keywords, adjusted to avoid subqueries in CASE
sql_query = """
WITH PreChecks AS (
    SELECT
        sd.sbsb_id,
        sd.effectivedate,
        sd.termdate,
        sd.CSPI_ID,
        MAX(CASE WHEN mepe.mepe_effectivedate <= sd.effectivedate AND sd.effectivedate <= mepe.mepe_termdate THEN 1 ELSE 0 END) AS InDateRange,
        MAX(CASE WHEN sd.effectivedate < mepe.mepe_effectivedate OR sd.effectivedate > mepe.mepe_termdate THEN 1 ELSE 0 END) AS OutOfDateRange,
        MAX(CASE WHEN mepe.CSPI_ID != sd.CSPI_ID OR mepe.CSCS_ID != sd.CSCS_ID THEN 1 ELSE 0 END) AS DifferentIds
    FROM
        sd_df sd
    LEFT JOIN
        mepe_df mepe ON mepe.sbsb_id = sd.sbsb_id
    GROUP BY
        sd.sbsb_id, sd.effectivedate, sd.termdate, sd.CSPI_ID
),
VoidKeywords AS (
    SELECT
        sbel.sbsb_id,
        CONCAT_WS('\r\n', COLLECT_LIST(
            CONCAT('@pSBEL_Update_CD="VC",@pSBEL_EffDate=', CAST(sbel.SBEL_EFF_DT AS STRING), ',@pSBE_MCTR_VRSN_NVL="CE"')
        )) AS VoidKeyword
    FROM
        sbel_df sbel
    WHERE
        sbel.SBEL_VOID_IND = 'N'
    GROUP BY
        sbel.sbsb_id
)
SELECT
    pc.sbsb_id AS SID,
    CONCAT(
        CASE
            WHEN pc.effectivedate = pc.termdate AND pc.InDateRange = 1 THEN CONCAT(
                '@pSBEL_EffDate=', CAST(DATE_SUB(pc.effectivedate, 1) AS STRING), ',@pSBEL_ELIG_TYPE="TM"',
                '\r\n', vk.VoidKeyword
            )
            WHEN pc.termdate > (SELECT MAX(mepe.mepe_termdate) FROM mepe_df mepe WHERE mepe.sbsb_id = pc.sbsb_id) THEN CONCAT(
                '@pSBEL_EffDate=', CAST(pc.termdate AS STRING), ',@pSBEL_ELIG_TYPE="TM"',
                '\r\n', vk.VoidKeyword
            )
            WHEN pc.DifferentIds = 1 THEN CONCAT('@pSBEL_EffDate=', CAST(pc.effectivedate AS STRING), ',@pSBEL_ELIG_TYPE="CH"')
            WHEN pc.OutOfDateRange = 1 THEN CONCAT('@pSBEL_EffDate=', CAST(pc.effectivedate AS STRING), ',@pSBEL_ELIG_TYPE="RI"')
            ELSE CONCAT('@pRECTYPE="SBEL",@pSBEL_Update_CD="IN",', '@pSBEL_EffDate="', CAST(pc.effectivedate AS STRING), '",@pSBEL_ELIG_TYPE="SL",', '@pCSPD_CAT="M",', '@pCSPI_ID="', pc.CSPI_ID, '",@pSBEL_FI ="C"')
        END AS SBEL_ConcatResult
FROM
    PreChecks pc
LEFT JOIN
    VoidKeywords vk ON vk.sbsb_id = pc.sbsb_id
"""

# Execute the query
keywords_df = spark.sql(sql_query)

# Show the result
keywords_df.show(truncate=False)

---=================
from pyspark.sql.functions import col, concat_ws, lit, when

# Expression to output specific strings for 'ENG' and 'SP', and 'O' for other cases
Concat_expr = concat_ws("",
    when((col("lan") == "ENG") | (col("lan") == "SP"),
        when(col("lan") == "ENG", concat_ws("", lit("@pLang=\""), col("lan"), lit("\"")))
        .otherwise(lit("@plan=s"))
    ).otherwise(lit("O")),  # Outputs 'O' if 'lan' is not 'ENG' or 'SP'
    lit("@pmbi"), col("mbi"), lit("\"")
)

# Selecting the expression with an alias
drawl = nmdm_df.select(Concat_expr.alias("nmdm_concatResult"))





