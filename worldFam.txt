from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Create Spark session
spark = SparkSession.builder.appName("table_merge_logic").getOrCreate()

# Assume sampleaData_df and span_df are your DataFrames with the respective columns

# Step 1: Append span_df values to sampleaData_df as new columns and create table3
table3 = sampleaData_df.join(span_df, sampleaData_df.MBINumber == span_df.MBINumber, "left_outer") \
    .select(sampleaData_df["*"], span_df["Stype"], span_df["S_EffDate"], span_df["S_TermDate"])

# Step 2: Add BLEI_ConcatResult column based on the described logic
table3 = table3.withColumn("BLEI_ConcatResult",
    F.when((F.col("Stype") != "MCAID") & ((F.col("Prem_Withhold_opt").isNotNull()) | (F.col("Prem_Withhold_opt") != "N")) &
           ((F.col("PartCPremium") + F.col("PartDPremium")) > 0),
           F.expr("""concat('@pBLPF_ID = "S4", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', termdate, '"')""")
          )
          .when((F.col("Stype") == "MCAID") & (F.col("S_EffDate") <= F.col("effectivedate")) & (F.col("S_TermDate") >= F.col("termdate")),
           F.expr("""concat('@pBLPF_ID = "M2", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', termdate, '"')""")
          )
          .when((F.col("Stype") == "MCAID") & (F.col("S_EffDate") <= F.col("effectivedate")) & (F.col("S_TermDate") < F.col("termdate")),
           F.expr("""concat('@pBLPF_ID = "M2", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', S_TermDate, '"')""")
          )
          .when((F.col("Stype") == "MCAID") & (F.col("S_EffDate") > F.col("effectivedate")) & (F.col("S_TermDate") >= F.col("termdate")),
           F.expr("""concat('@pBLPF_ID = "S1", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', date_sub(S_EffDate, 1), '"')""")
          )
          .when((F.col("Stype") == "MCAID") & (F.col("S_EffDate") > F.col("effectivedate")) & (F.col("S_TermDate") < F.col("termdate")),
           F.expr("""concat('@pBLPF_ID = "S1", @pBLBP_EFF_DT = "', date_add(S_TermDate, 1), '", @pBLBP_TERM_DT = "', termdate, '"')""")
          )
          .otherwise(F.expr("""concat('@pBLPF_ID = "S1", @pBLBP_EFF_DT = "', effectivedate, '", @pBLBP_TERM_DT = "', termdate, '"')""")
)

# Show the final DataFrame
table3.show(truncate=False)

# Stop the Spark session
spark.stop()
--===================
from pyspark.sql import SparkSession
from pyspark.sql.functions import concat_ws, col, lit, when

# Create SparkSession
spark = SparkSession.builder \
    .appName("Conditional Key-Value Pairs") \
    .getOrCreate()

# Assuming you have already loaded your input data into a DataFrame named 'nmdm_dftable'

# Define your key-value pairs with conditions
concat_expr = concat_ws(",", 
                        lit("@ppbpId="), col("pdbId"), 
                        lit(",@pSegmentId="), col("segmentId"), 
                        lit(",@pA_EffDate="), 
                        when(col("AeffDate").isNotNull(), col("AeffDate")).otherwise(lit("")), 
                        lit(",@pB_EffDate="), 
                        when(col("BeffDate").isNotNull(), col("BeffDate")).otherwise(lit("")), 
                        lit(",@pC_EffDate="), 
                        when(col("CeffDate").isNotNull(), col("CeffDate")).otherwise(lit(""))
                        # Add similar conditions for other optional columns
                        )

# Select subID as SID and the concatenated result with conditional key-value pairs
result_df = nmdm_dftable.select(col("subID").alias("SID"), concat_expr.alias("ConcatResult"))

# Now you can proceed to create a table from 'result_df' or perform further operations as needed



