Hello world
2

import urllib.request
import urllib.parse
import json
import base64

def encode_credentials(username, password):
    credentials = f"{username}:{password}"
    encoded_credentials = base64.b64encode(credentials.encode()).decode()
    return encoded_credentials

def make_api_call(username, password):
    url = 'https://api.example.com/token'
    data = urllib.parse.urlencode({'grant_type': 'client'}).encode()
    encoded_credentials = encode_credentials(username, password)
    headers = {
        'Authorization': f'Basic {encoded_credentials}',
        'Content-Type': 'application/x-www-form-urlencoded'
    }

    req = urllib.request.Request(url, data=data, headers=headers, method='POST')

    try:
        with urllib.request.urlopen(req) as response:
            if response.status == 200:
                # Process the API response data here
                api_data = json.loads(response.read().decode())
                bearer_token = api_data.get('access_token')
                return bearer_token
            else:
                print("Failed to make API call. Status code:", response.status)
                return None
    except urllib.error.URLError as e:
        print("Failed to make API call:", e.reason)
        return None

# Replace 'hello' and 'world02A51cA' with your actual username and password
bearer_token = make_api_call('hello', 'world02A51cA')
print("Bearer token:", bearer_token)

--======================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("FixedWidthFileReader") \
    .getOrCreate()

# Read the text file from S3 and remove header and footer
file_path = "s3://your_bucket/your_file.txt"
df = spark.read.text(file_path)
df = df.filter(~col("value").startswith("***") & ~col("value").endswith("***"))

# Define the fixed widths of each column
column_widths = [(0, 5), (6, 14), (15, 23)]  # Adjust based on your actual column widths

# Define column names
header = df.first()
column_names = header.value.split()

# Parse the fixed-width lines and create DataFrame
for i, (start, end) in enumerate(column_widths):
    df = df.withColumn(f"col_{i}", df["value"].substr(start + 1, end - start))

# Drop the original 'value' column
df = df.drop("value")

# Rename columns with proper column names
for i, name in enumerate(column_names):
    df = df.withColumnRenamed(f"col_{i}", name)

# Create a temporary view
df.createOrReplaceTempView("fixed_width_data")

# Query the data using Spark SQL
result = spark.sql("SELECT * FROM fixed_width_data")

# Show result
result.show()

# Stop SparkSession
spark.stop()



--========••••••=••============
from pyspark.sql import SparkSession
from pyspark.sql.functions import substring, lit

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("FixedWidthFileReader") \
    .getOrCreate()

# Read the text file from S3
file_path = "s3://your_bucket/your_file.txt"
lines = spark.read.text(file_path).rdd.map(lambda r: r[0])

# Filter out header and footer lines
data_lines = lines.filter(~lines.startswith("***") & ~lines.startswith("****") & ~lines.startswith("**"))

# Define column positions and lengths for the column name row
column_name_positions_lengths = [
    (1, 5),
    (6, 10),
    (21, 15),
    (36, 5),
    (41, 15),
    (56, 8),
    (64, 8)
]

# Parse the column name row
columns = []
for start, length in column_name_positions_lengths:
    columns.append(substring("value", start, length).alias("col_" + str(start)))

# Create DataFrame for column names
column_names_row = data_lines.filter(data_lines.startswith("H0354MBI")).map(lambda line: line).toDF()
column_names_row = column_names_row.select(*columns)

# Define column positions and lengths for the data row
data_positions_lengths = [
    (1, 5),
    (6, 10),
    (21, 15),
    (36, 5),
    (41, 15),
    (56, 8),
    (64, 8)
]

# Parse the data row
for i, (start, length) in enumerate(data_positions_lengths):
    column_name = f"col_{start}"
    data_lines = data_lines.withColumn(column_name, substring("value", start, length))

# Drop the original 'value' column
data_lines = data_lines.drop("value")

# Combine the data with column names
data_with_column_names = data_lines.union(column_names_row)

# Show DataFrame
data_with_column_names.show()

--==========================
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Read Fixed Width File") \
    .getOrCreate()

# Define the schema based on column positions and lengths
schema = "PlanID string, HNumber string, MNumber string, SType string, Val string, Startdate string, EndDate string"

# Define column names and their positions and lengths
columns = [
    ("PlanID", 1, 5),
    ("HNumber", 6, 15),
    ("MNumber", 21, 5),
    ("SType", 36, 5),
    ("Val", 41, 15),
    ("Startdate", 56, 8),
    ("EndDate", 64, 8)
]

# Read the fixed width file, skipping the first 2 rows (header and column names)
df = spark.read.text("s3://your_bucket_name/your_file.txt") \
    .rdd \
    .map(lambda row: [row.value[start-1:start+length-1].strip() for name, start, length in columns]) \
    .toDF(schema)

# Show the dataframe
df.show()
