Hello world


import urllib.request
import urllib.parse
import json
import base64

def encode_credentials(username, password):
    credentials = f"{username}:{password}"
    encoded_credentials = base64.b64encode(credentials.encode()).decode()
    return encoded_credentials

def make_api_call(username, password):
    url = 'https://api.example.com/token'
    data = urllib.parse.urlencode({'grant_type': 'client'}).encode()
    encoded_credentials = encode_credentials(username, password)
    headers = {
        'Authorization': f'Basic {encoded_credentials}',
        'Content-Type': 'application/x-www-form-urlencoded'
    }

    req = urllib.request.Request(url, data=data, headers=headers, method='POST')

    try:
        with urllib.request.urlopen(req) as response:
            if response.status == 200:
                # Process the API response data here
                api_data = json.loads(response.read().decode())
                bearer_token = api_data.get('access_token')
                return bearer_token
            else:
                print("Failed to make API call. Status code:", response.status)
                return None
    except urllib.error.URLError as e:
        print("Failed to make API call:", e.reason)
        return None

# Replace 'hello' and 'world02A51cA' with your actual username and password
bearer_token = make_api_call('hello', 'world02A51cA')
print("Bearer token:", bearer_token)

--======================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("FixedWidthFileReader") \
    .getOrCreate()

# Read the text file from S3 and remove header and footer
file_path = "s3://your_bucket/your_file.txt"
df = spark.read.text(file_path)
df = df.filter(~col("value").startswith("***") & ~col("value").endswith("***"))

# Define the fixed widths of each column
column_widths = [(0, 5), (6, 14), (15, 23)]  # Adjust based on your actual column widths

# Define column names
header = df.first()
column_names = header.value.split()

# Parse the fixed-width lines and create DataFrame
for i, (start, end) in enumerate(column_widths):
    df = df.withColumn(f"col_{i}", df["value"].substr(start + 1, end - start))

# Drop the original 'value' column
df = df.drop("value")

# Rename columns with proper column names
for i, name in enumerate(column_names):
    df = df.withColumnRenamed(f"col_{i}", name)

# Create a temporary view
df.createOrReplaceTempView("fixed_width_data")

# Query the data using Spark SQL
result = spark.sql("SELECT * FROM fixed_width_data")

# Show result
result.show()

# Stop SparkSession
spark.stop()
